{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f170519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import psycopg\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from sqlalchemy import create_engine, Column, String, Integer, Text\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fb4a68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Engine Land - News, Search Engine Optimization (SEO), Pay-Per-Click (PPC)\n",
      "Starting to scrape Search Engine Land for article headlines...\n",
      "Scraping page 1/10: https://searchengineland.com/\n",
      "Found 42 new articles on https://searchengineland.com/\n",
      "Scraping page 2/10: https://searchengineland.com/page/2\n",
      "Found 36 new articles on https://searchengineland.com/page/2\n",
      "Scraping page 3/10: https://searchengineland.com/page/3\n",
      "Found 35 new articles on https://searchengineland.com/page/3\n",
      "Scraping page 4/10: https://searchengineland.com/page/4\n",
      "Found 37 new articles on https://searchengineland.com/page/4\n",
      "Scraping page 5/10: https://searchengineland.com/page/5\n",
      "Found 40 new articles on https://searchengineland.com/page/5\n",
      "Scraping page 6/10: https://searchengineland.com/page/2/\n",
      "Found 0 new articles on https://searchengineland.com/page/2/\n",
      "Scraping page 7/10: https://searchengineland.com/page/3/\n",
      "Found 0 new articles on https://searchengineland.com/page/3/\n",
      "Scraping page 8/10: https://searchengineland.com/page/4/\n",
      "Found 0 new articles on https://searchengineland.com/page/4/\n",
      "Scraping page 9/10: https://searchengineland.com/page/5/\n",
      "Found 0 new articles on https://searchengineland.com/page/5/\n",
      "Scraping page 10/10: https://searchengineland.com/page/6\n",
      "Found 39 new articles on https://searchengineland.com/page/6\n",
      "Completed! Scraped 10 pages and found 229 unique articles.\n",
      "Data saved to search_engine_land_articles.csv\n",
      "\n",
      "Sample of articles found:\n",
      "1. > Local SEO predictions 2025\n",
      "   https://searchengineland.com/guide/local-seo-in-2025\n",
      "\n",
      "2. Dealing with Google Ads frustrations: Poor support, suspensions, rising costs\n",
      "   https://searchengineland.com/google-ads-frustrations-support-suspensions-costs-453947\n",
      "\n",
      "3. Google launches automatic marketing content extraction\n",
      "   https://searchengineland.com/google-automatic-marketing-content-extraction-453914\n",
      "\n",
      "4. Google vision match vs. traditional search: Early insights on AI shopping tool\n",
      "   https://searchengineland.com/google-vision-match-traditional-search-454046\n",
      "\n",
      "5. Your 2025 playbook for AI-powered cross-channel brand visibility\n",
      "   https://searchengineland.com/your-2025-playbook-for-ai-powered-cross-channel-brand-visibility-454026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the base URL for Search Engine Land\n",
    "base_url = 'https://searchengineland.com/'\n",
    "\n",
    "# Create headers to simulate a browser (prevents being blocked)\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "}\n",
    "\n",
    "# Get the main page first\n",
    "response = requests.get(base_url, headers=headers)\n",
    "soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Print the title of the page to confirm it worked\n",
    "print(soup.title.text)\n",
    "\n",
    "# Create lists to store our data\n",
    "article_titles = []\n",
    "article_urls = []\n",
    "\n",
    "# Function to check if a URL is a valid article URL\n",
    "def is_article_url(url):\n",
    "    # Skip author pages\n",
    "    if '/author/' in url:\n",
    "        return False\n",
    "        \n",
    "    # Skip category/library pages\n",
    "    if '/category/' in url or '/library/' in url:\n",
    "        return False\n",
    "        \n",
    "    # Skip tag pages\n",
    "    if '/tag/' in url:\n",
    "        return False\n",
    "        \n",
    "    # Skip about/contact/advertise pages\n",
    "    if any(x in url for x in ['/about', '/contact', '/advertise', '/jobs']):\n",
    "        return False\n",
    "        \n",
    "    # Skip archive pages\n",
    "    if re.search(r'/\\d{4}/\\d{2}/', url):\n",
    "        return False\n",
    "        \n",
    "    # Skip URLs with just numbers at the end (likely pagination)\n",
    "    if re.search(r'/\\d+/?$', url):\n",
    "        return False\n",
    "        \n",
    "    # Accept URLs that end with a numeric ID (likely an article)\n",
    "    # Example: https://searchengineland.com/google-drops-ai-while-browsing-feature-453671\n",
    "    if re.search(r'-\\d+/?$', url):\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "# Function to extract article headlines and links from a page\n",
    "def extract_articles(url):\n",
    "    # Get the page content\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Check if request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to access {url}, status code: {response.status_code}\")\n",
    "        return\n",
    "    \n",
    "    # Parse the HTML\n",
    "    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all links\n",
    "    links = soup.find_all('a', href=True)\n",
    "    \n",
    "    initial_count = len(article_urls)\n",
    "    \n",
    "    # Process each link\n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "        \n",
    "        # Check if it's a Search Engine Land URL\n",
    "        if 'searchengineland.com' in href:\n",
    "            # Check if it matches our article URL pattern\n",
    "            if is_article_url(href):\n",
    "                title = link.get_text(strip=True)\n",
    "                \n",
    "                # Skip empty or very short titles\n",
    "                if not title or len(title) < 10:\n",
    "                    continue\n",
    "                    \n",
    "                # Only add if not already in our list\n",
    "                if href not in article_urls:\n",
    "                    article_titles.append(title)\n",
    "                    article_urls.append(href)\n",
    "    \n",
    "    print(f\"Found {len(article_urls) - initial_count} new articles on {url}\")\n",
    "\n",
    "# Start with the homepage\n",
    "print(\"Starting to scrape Search Engine Land for article headlines...\")\n",
    "urls_to_visit = [base_url]\n",
    "already_visited = set()\n",
    "\n",
    "# Set limit for number of pages to scrape\n",
    "max_pages = 10\n",
    "pages_scraped = 0\n",
    "\n",
    "# Scrape pages until we reach our limit\n",
    "while urls_to_visit and pages_scraped < max_pages:\n",
    "    # Get the next URL\n",
    "    current_url = urls_to_visit.pop(0)\n",
    "    \n",
    "    # Skip if already visited\n",
    "    if current_url in already_visited:\n",
    "        continue\n",
    "        \n",
    "    # Mark as visited\n",
    "    already_visited.add(current_url)\n",
    "    pages_scraped += 1\n",
    "    \n",
    "    print(f\"Scraping page {pages_scraped}/{max_pages}: {current_url}\")\n",
    "    \n",
    "    # Add delay to be respectful to the server\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Extract articles from the page\n",
    "    extract_articles(current_url)\n",
    "    \n",
    "    # Get the page content to find more URLs\n",
    "    response = requests.get(current_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find links to category pages and pagination pages\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            \n",
    "            # Add category pages but not author or tag pages\n",
    "            if ('searchengineland.com' in href and \n",
    "                (('/category/' in href) or ('/page/' in href)) and\n",
    "                href not in already_visited and \n",
    "                href not in urls_to_visit):\n",
    "                urls_to_visit.append(href)\n",
    "                \n",
    "        # Also try to find pagination links by pattern matching\n",
    "        # Look for /page/2, /page/3, etc.\n",
    "        if '/page/' not in current_url:  # Only do this if we're not already on a paginated page\n",
    "            base_path = current_url.rstrip('/')\n",
    "            for page_num in range(2, 6):  # Check pages 2 through 5\n",
    "                pagination_url = f\"{base_path}/page/{page_num}/\"\n",
    "                if pagination_url not in already_visited and pagination_url not in urls_to_visit:\n",
    "                    urls_to_visit.append(pagination_url)\n",
    "\n",
    "# Create a DataFrame with our data\n",
    "articles_df = pd.DataFrame({\n",
    "    'title': article_titles,\n",
    "    'url': article_urls\n",
    "})\n",
    "\n",
    "# Remove duplicates\n",
    "articles_df = articles_df.drop_duplicates(subset=['url'])\n",
    "\n",
    "# Save to CSV (keeping this for backup)\n",
    "articles_df.to_csv(\"search_engine_land_articles.csv\", index=False)\n",
    "\n",
    "print(f\"Completed! Scraped {pages_scraped} pages and found {len(articles_df)} unique articles.\")\n",
    "print(f\"Data saved to search_engine_land_articles.csv\")\n",
    "\n",
    "# Print a sample of the articles found\n",
    "print(\"\\nSample of articles found:\")\n",
    "for i, (title, url) in enumerate(zip(articles_df['title'].head(5), articles_df['url'].head(5))):\n",
    "    print(f\"{i+1}. {title}\")\n",
    "    print(f\"   {url}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAB_V1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
