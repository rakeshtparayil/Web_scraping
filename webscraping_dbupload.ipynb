{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36955c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database setup complete.\n",
      "Starting to scrape: Search Engine Land - News, Search Engine Optimization (SEO), Pay-Per-Click (PPC)\n",
      "Starting to scrape Search Engine Land for article headlines...\n",
      "Scraping page 1/5: https://searchengineland.com/\n",
      "Found 42 new articles on https://searchengineland.com/\n",
      "Saved 42 articles to database.\n",
      "Scraping page 2/5: https://searchengineland.com/page/2\n",
      "Found 38 new articles on https://searchengineland.com/page/2\n",
      "Saved 38 articles to database.\n",
      "Scraping page 3/5: https://searchengineland.com/page/3\n",
      "Found 36 new articles on https://searchengineland.com/page/3\n",
      "Saved 36 articles to database.\n",
      "Scraping page 4/5: https://searchengineland.com/page/4\n",
      "Found 38 new articles on https://searchengineland.com/page/4\n",
      "Saved 38 articles to database.\n",
      "Scraping page 5/5: https://searchengineland.com/page/5\n",
      "Found 41 new articles on https://searchengineland.com/page/5\n",
      "Saved 41 articles to database.\n",
      "Completed! Scraped 5 pages and found 190 unique articles.\n",
      "\n",
      "Sample of articles found:\n",
      "1. > Local SEO predictions 2025\n",
      "   https://searchengineland.com/guide/local-seo-in-2025\n",
      "\n",
      "2. Dealing with Google Ads frustrations: Poor support, suspensions, rising costs\n",
      "   https://searchengineland.com/google-ads-frustrations-support-suspensions-costs-453947\n",
      "\n",
      "3. Google launches automatic marketing content extraction\n",
      "   https://searchengineland.com/google-automatic-marketing-content-extraction-453914\n",
      "\n",
      "4. Google vision match vs. traditional search: Early insights on AI shopping tool\n",
      "   https://searchengineland.com/google-vision-match-traditional-search-454046\n",
      "\n",
      "5. Your 2025 playbook for AI-powered cross-channel brand visibility\n",
      "   https://searchengineland.com/your-2025-playbook-for-ai-powered-cross-channel-brand-visibility-454026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from datetime import datetime\n",
    "\n",
    "# Database configuration - will use environment variables in Railway\n",
    "def get_db_connection():\n",
    "    \"\"\"Create a connection to the PostgreSQL database\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=os.environ.get('PGHOST', 'localhost'),\n",
    "            database=os.environ.get('PGDATABASE', 'postgres'),\n",
    "            user=os.environ.get('PGUSER', 'postgres'),\n",
    "            password=os.environ.get('PGPASSWORD', 'postgres'),\n",
    "            port=os.environ.get('PGPORT', '5432')\n",
    "        )\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Database connection error: {e}\")\n",
    "        return None\n",
    "\n",
    "def setup_database():\n",
    "    \"\"\"Create the necessary table if it doesn't exist\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    if not conn:\n",
    "        print(\"Failed to connect to database, exiting.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            # Create articles table if it doesn't exist\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS articles (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    title TEXT NOT NULL,\n",
    "                    url TEXT UNIQUE NOT NULL,\n",
    "                    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                );\n",
    "            \"\"\")\n",
    "            conn.commit()\n",
    "            print(\"Database setup complete.\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"Database setup error: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def save_to_database(articles_data):\n",
    "    \"\"\"Save the scraped articles to the PostgreSQL database\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    if not conn:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            # Insert articles using a single query for better performance\n",
    "            insert_query = sql.SQL(\"\"\"\n",
    "                INSERT INTO articles (title, url, scraped_at)\n",
    "                VALUES (%s, %s, %s)\n",
    "                ON CONFLICT (url) \n",
    "                DO UPDATE SET \n",
    "                    title = EXCLUDED.title,\n",
    "                    scraped_at = EXCLUDED.scraped_at\n",
    "            \"\"\")\n",
    "            \n",
    "            # Current timestamp for all insertions\n",
    "            now = datetime.now()\n",
    "            \n",
    "            # Prepare data for insertion\n",
    "            values = [(title, url, now) for title, url in articles_data]\n",
    "            \n",
    "            # Execute the query for all articles\n",
    "            cur.executemany(insert_query, values)\n",
    "            conn.commit()\n",
    "            \n",
    "            print(f\"Saved {len(articles_data)} articles to database.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Database insertion error: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Set the base URL for Search Engine Land\n",
    "base_url = 'https://searchengineland.com/'\n",
    "\n",
    "# Create headers to simulate a browser (prevents being blocked)\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "}\n",
    "\n",
    "# Function to check if a URL is a valid article URL\n",
    "def is_article_url(url):\n",
    "    # Skip author pages\n",
    "    if '/author/' in url:\n",
    "        return False\n",
    "        \n",
    "    # Skip category/library pages\n",
    "    if '/category/' in url or '/library/' in url:\n",
    "        return False\n",
    "        \n",
    "    # Skip tag pages\n",
    "    if '/tag/' in url:\n",
    "        return False\n",
    "        \n",
    "    # Skip about/contact/advertise pages\n",
    "    if any(x in url for x in ['/about', '/contact', '/advertise', '/jobs']):\n",
    "        return False\n",
    "        \n",
    "    # Skip archive pages\n",
    "    if re.search(r'/\\d{4}/\\d{2}/', url):\n",
    "        return False\n",
    "        \n",
    "    # Skip URLs with just numbers at the end (likely pagination)\n",
    "    if re.search(r'/\\d+/?$', url):\n",
    "        return False\n",
    "        \n",
    "    # Accept URLs that end with a numeric ID (likely an article)\n",
    "    # Example: https://searchengineland.com/google-drops-ai-while-browsing-feature-453671\n",
    "    if re.search(r'-\\d+/?$', url):\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "# Function to extract article headlines and links from a page\n",
    "def extract_articles(url):\n",
    "    # Get the page content\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to access {url}: {e}\")\n",
    "        return [], []\n",
    "    \n",
    "    # Parse the HTML\n",
    "    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all links\n",
    "    links = soup.find_all('a', href=True)\n",
    "    \n",
    "    found_titles = []\n",
    "    found_urls = []\n",
    "    \n",
    "    # Process each link\n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "        \n",
    "        # Check if it's a Search Engine Land URL\n",
    "        if 'searchengineland.com' in href:\n",
    "            # Check if it matches our article URL pattern\n",
    "            if is_article_url(href):\n",
    "                title = link.get_text(strip=True)\n",
    "                \n",
    "                # Skip empty or very short titles\n",
    "                if not title or len(title) < 10:\n",
    "                    continue\n",
    "                    \n",
    "                # Only add if not already in our list\n",
    "                if href not in found_urls:\n",
    "                    found_titles.append(title)\n",
    "                    found_urls.append(href)\n",
    "    \n",
    "    print(f\"Found {len(found_urls)} new articles on {url}\")\n",
    "    return found_titles, found_urls\n",
    "\n",
    "def scrape_search_engine_land():\n",
    "    \"\"\"Main function to scrape Search Engine Land\"\"\"\n",
    "    # Setup database first\n",
    "    if not setup_database():\n",
    "        return\n",
    "    \n",
    "    # Get the main page first\n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to access {base_url}: {e}\")\n",
    "        return\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "    print(f\"Starting to scrape: {soup.title.text}\")\n",
    "    \n",
    "    # Create lists to store our data\n",
    "    all_article_titles = []\n",
    "    all_article_urls = []\n",
    "    \n",
    "    # Start with the homepage\n",
    "    print(\"Starting to scrape Search Engine Land for article headlines...\")\n",
    "    urls_to_visit = [base_url]\n",
    "    already_visited = set()\n",
    "    \n",
    "    # Set limit for number of pages to scrape\n",
    "    max_pages = int(os.environ.get('MAX_PAGES', 5))  # Default changed to 5 pages\n",
    "    pages_scraped = 0\n",
    "    \n",
    "    # Scrape pages until we reach our limit\n",
    "    while urls_to_visit and pages_scraped < max_pages:\n",
    "        # Get the next URL\n",
    "        current_url = urls_to_visit.pop(0)\n",
    "        \n",
    "        # Skip if already visited\n",
    "        if current_url in already_visited:\n",
    "            continue\n",
    "            \n",
    "        # Mark as visited\n",
    "        already_visited.add(current_url)\n",
    "        pages_scraped += 1\n",
    "        \n",
    "        print(f\"Scraping page {pages_scraped}/{max_pages}: {current_url}\")\n",
    "        \n",
    "        # Add delay to be respectful to the server\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Extract articles from the page\n",
    "        titles, urls = extract_articles(current_url)\n",
    "        \n",
    "        # Add to our master lists\n",
    "        all_article_titles.extend(titles)\n",
    "        all_article_urls.extend(urls)\n",
    "        \n",
    "        # Save to database in batches\n",
    "        if len(titles) > 0:\n",
    "            article_data = list(zip(titles, urls))\n",
    "            save_to_database(article_data)\n",
    "        \n",
    "        # Get the page content to find more URLs\n",
    "        try:\n",
    "            response = requests.get(current_url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find links to category pages and pagination pages\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            \n",
    "            # Add category pages but not author or tag pages\n",
    "            if ('searchengineland.com' in href and \n",
    "                (('/category/' in href) or ('/page/' in href)) and\n",
    "                href not in already_visited and \n",
    "                href not in urls_to_visit):\n",
    "                urls_to_visit.append(href)\n",
    "                \n",
    "        # Also try to find pagination links by pattern matching\n",
    "        # Look for /page/2, /page/3, etc.\n",
    "        if '/page/' not in current_url:  # Only do this if we're not already on a paginated page\n",
    "            base_path = current_url.rstrip('/')\n",
    "            for page_num in range(2, 6):  # Check pages 2 through 5\n",
    "                pagination_url = f\"{base_path}/page/{page_num}/\"\n",
    "                if pagination_url not in already_visited and pagination_url not in urls_to_visit:\n",
    "                    urls_to_visit.append(pagination_url)\n",
    "    \n",
    "    # Create a DataFrame for reporting purposes\n",
    "    articles_df = pd.DataFrame({\n",
    "        'title': all_article_titles,\n",
    "        'url': all_article_urls\n",
    "    })\n",
    "    \n",
    "    # Remove duplicates\n",
    "    articles_df = articles_df.drop_duplicates(subset=['url'])\n",
    "    \n",
    "    print(f\"Completed! Scraped {pages_scraped} pages and found {len(articles_df)} unique articles.\")\n",
    "    \n",
    "    # Print a sample of the articles found\n",
    "    print(\"\\nSample of articles found:\")\n",
    "    for i, (title, url) in enumerate(zip(articles_df['title'].head(5), articles_df['url'].head(5))):\n",
    "        print(f\"{i+1}. {title}\")\n",
    "        print(f\"   {url}\")\n",
    "        print()\n",
    "\n",
    "# Run the scraper if this is the main file\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_search_engine_land()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAB_V1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
