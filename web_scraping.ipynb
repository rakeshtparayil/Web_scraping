{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    }
   ],
   "source": [
    "print(\"testing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import psycopg\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from sqlalchemy import create_engine, Column, String, Integer, Text\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 1/5: https://ï»¿https://dubaihousekeeping.com/\n",
      "Scraping 2/5: https://elitemaids.ae/\n",
      "Scraping 3/5: https://justmaid.ae/\n",
      "Scraping 4/5: https://servicemarket.com/en/dubai/cleaning-maid-services\n",
      "Scraping 5/5: https://dubaiclean.com/\n",
      "\n",
      "Done! Results saved to 'meta_scraping_results.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === Start of Script ===\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"/Users/rakesh/Downloads/cleaning_company.csv\"              # Replace with your input CSV file\n",
    "    output_file = \"meta_scraping_results.csv\"\n",
    "\n",
    "    # Set up browser\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Read URLs from CSV\n",
    "    urls = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                url = row[0].strip()\n",
    "                if not url.startswith(\"http\"):\n",
    "                    url = \"https://\" + url\n",
    "                urls.append(url)\n",
    "\n",
    "    # Scrape data\n",
    "    results = []\n",
    "    for i, url in enumerate(urls):\n",
    "        print(f\"Scraping {i + 1}/{len(urls)}: {url}\")\n",
    "        data = {\n",
    "            'url': url,\n",
    "            'title': '',\n",
    "            'meta_description': '',\n",
    "            'meta_keywords': '',\n",
    "            'status': 'success'\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "\n",
    "            data['title'] = driver.title\n",
    "\n",
    "            try:\n",
    "                desc = driver.find_element(By.XPATH, \"//meta[@name='description']\")\n",
    "                data['meta_description'] = desc.get_attribute('content')\n",
    "            except:\n",
    "                data['meta_description'] = 'Not found'\n",
    "\n",
    "            try:\n",
    "                keywords = driver.find_element(By.XPATH, \"//meta[@name='keywords']\")\n",
    "                data['meta_keywords'] = keywords.get_attribute('content')\n",
    "            except:\n",
    "                data['meta_keywords'] = 'Not found'\n",
    "\n",
    "        except Exception as e:\n",
    "            data['status'] = f'error: {str(e)}'\n",
    "\n",
    "        results.append(data)\n",
    "\n",
    "    # Save to CSV\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['url', 'title', 'meta_description', 'meta_keywords', 'status'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "    # Close browser\n",
    "    driver.quit()\n",
    "    print(f\"\\nDone! Results saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Engine Land - News, Search Engine Optimization (SEO), Pay-Per-Click (PPC)\n",
      "Starting to scrape Search Engine Land for article headlines...\n",
      "Scraping page 1/10: https://searchengineland.com/\n",
      "Found 41 new articles on https://searchengineland.com/\n",
      "Scraping page 2/10: https://searchengineland.com/page/2\n",
      "Found 39 new articles on https://searchengineland.com/page/2\n",
      "Scraping page 3/10: https://searchengineland.com/page/3\n",
      "Found 40 new articles on https://searchengineland.com/page/3\n",
      "Scraping page 4/10: https://searchengineland.com/page/4\n",
      "Found 40 new articles on https://searchengineland.com/page/4\n",
      "Scraping page 5/10: https://searchengineland.com/page/5\n",
      "Found 39 new articles on https://searchengineland.com/page/5\n",
      "Scraping page 6/10: https://searchengineland.com/page/2/\n",
      "Found 1 new articles on https://searchengineland.com/page/2/\n",
      "Scraping page 7/10: https://searchengineland.com/page/3/\n",
      "Found 0 new articles on https://searchengineland.com/page/3/\n",
      "Scraping page 8/10: https://searchengineland.com/page/4/\n",
      "Found 0 new articles on https://searchengineland.com/page/4/\n",
      "Scraping page 9/10: https://searchengineland.com/page/5/\n",
      "Found 0 new articles on https://searchengineland.com/page/5/\n",
      "Scraping page 10/10: https://searchengineland.com/page/6\n",
      "Found 39 new articles on https://searchengineland.com/page/6\n",
      "Completed! Scraped 10 pages and found 239 unique articles.\n",
      "Data saved to search_engine_land_articles.csv\n",
      "\n",
      "Sample of articles found:\n",
      "1. > Local SEO predictions 2025\n",
      "   https://searchengineland.com/guide/local-seo-in-2025\n",
      "\n",
      "2. Google March 2025 core update rollout is now complete\n",
      "   https://searchengineland.com/google-march-2025-core-update-rollout-is-now-complete-453364\n",
      "\n",
      "3. Meet LLMs.txt, a proposed standard for AI website content crawling\n",
      "   https://searchengineland.com/llms-txt-proposed-standard-453676\n",
      "\n",
      "4. Google settles $100 Million advertising dispute\n",
      "   https://searchengineland.com/google-settles-100-million-advertising-dispute-453706\n",
      "\n",
      "5. Maximize Conversion Value: Google Ads bidding explained\n",
      "   https://searchengineland.com/maximize-conversion-value-google-ads-bidding-explained-453638\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the base URL for Search Engine Land\n",
    "base_url = 'https://searchengineland.com/'\n",
    "\n",
    "# Create headers to simulate a browser (prevents being blocked)\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "}\n",
    "\n",
    "# Get the main page first\n",
    "response = requests.get(base_url, headers=headers)\n",
    "soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Print the title of the page to confirm it worked\n",
    "print(soup.title.text)\n",
    "\n",
    "# Create lists to store our data\n",
    "article_titles = []\n",
    "article_urls = []\n",
    "\n",
    "# Function to check if a URL is a valid article URL\n",
    "def is_article_url(url):\n",
    "    # Skip author pages\n",
    "    if '/author/' in url:\n",
    "        return False\n",
    "        \n",
    "    # Skip category/library pages\n",
    "    if '/category/' in url or '/library/' in url:\n",
    "        return False\n",
    "        \n",
    "    # Skip tag pages\n",
    "    if '/tag/' in url:\n",
    "        return False\n",
    "        \n",
    "    # Skip about/contact/advertise pages\n",
    "    if any(x in url for x in ['/about', '/contact', '/advertise', '/jobs']):\n",
    "        return False\n",
    "        \n",
    "    # Skip archive pages\n",
    "    if re.search(r'/\\d{4}/\\d{2}/', url):\n",
    "        return False\n",
    "        \n",
    "    # Skip URLs with just numbers at the end (likely pagination)\n",
    "    if re.search(r'/\\d+/?$', url):\n",
    "        return False\n",
    "        \n",
    "    # Accept URLs that end with a numeric ID (likely an article)\n",
    "    # Example: https://searchengineland.com/google-drops-ai-while-browsing-feature-453671\n",
    "    if re.search(r'-\\d+/?$', url):\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "# Function to extract article headlines and links from a page\n",
    "def extract_articles(url):\n",
    "    # Get the page content\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Check if request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to access {url}, status code: {response.status_code}\")\n",
    "        return\n",
    "    \n",
    "    # Parse the HTML\n",
    "    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all links\n",
    "    links = soup.find_all('a', href=True)\n",
    "    \n",
    "    initial_count = len(article_urls)\n",
    "    \n",
    "    # Process each link\n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "        \n",
    "        # Check if it's a Search Engine Land URL\n",
    "        if 'searchengineland.com' in href:\n",
    "            # Check if it matches our article URL pattern\n",
    "            if is_article_url(href):\n",
    "                title = link.get_text(strip=True)\n",
    "                \n",
    "                # Skip empty or very short titles\n",
    "                if not title or len(title) < 10:\n",
    "                    continue\n",
    "                    \n",
    "                # Only add if not already in our list\n",
    "                if href not in article_urls:\n",
    "                    article_titles.append(title)\n",
    "                    article_urls.append(href)\n",
    "    \n",
    "    print(f\"Found {len(article_urls) - initial_count} new articles on {url}\")\n",
    "\n",
    "# Start with the homepage\n",
    "print(\"Starting to scrape Search Engine Land for article headlines...\")\n",
    "urls_to_visit = [base_url]\n",
    "already_visited = set()\n",
    "\n",
    "# Set limit for number of pages to scrape\n",
    "max_pages = 10\n",
    "pages_scraped = 0\n",
    "\n",
    "# Scrape pages until we reach our limit\n",
    "while urls_to_visit and pages_scraped < max_pages:\n",
    "    # Get the next URL\n",
    "    current_url = urls_to_visit.pop(0)\n",
    "    \n",
    "    # Skip if already visited\n",
    "    if current_url in already_visited:\n",
    "        continue\n",
    "        \n",
    "    # Mark as visited\n",
    "    already_visited.add(current_url)\n",
    "    pages_scraped += 1\n",
    "    \n",
    "    print(f\"Scraping page {pages_scraped}/{max_pages}: {current_url}\")\n",
    "    \n",
    "    # Add delay to be respectful to the server\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Extract articles from the page\n",
    "    extract_articles(current_url)\n",
    "    \n",
    "    # Get the page content to find more URLs\n",
    "    response = requests.get(current_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find links to category pages and pagination pages\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            \n",
    "            # Add category pages but not author or tag pages\n",
    "            if ('searchengineland.com' in href and \n",
    "                (('/category/' in href) or ('/page/' in href)) and\n",
    "                href not in already_visited and \n",
    "                href not in urls_to_visit):\n",
    "                urls_to_visit.append(href)\n",
    "                \n",
    "        # Also try to find pagination links by pattern matching\n",
    "        # Look for /page/2, /page/3, etc.\n",
    "        if '/page/' not in current_url:  # Only do this if we're not already on a paginated page\n",
    "            base_path = current_url.rstrip('/')\n",
    "            for page_num in range(2, 6):  # Check pages 2 through 5\n",
    "                pagination_url = f\"{base_path}/page/{page_num}/\"\n",
    "                if pagination_url not in already_visited and pagination_url not in urls_to_visit:\n",
    "                    urls_to_visit.append(pagination_url)\n",
    "\n",
    "# Create a DataFrame with our data\n",
    "articles_df = pd.DataFrame({\n",
    "    'title': article_titles,\n",
    "    'url': article_urls\n",
    "})\n",
    "\n",
    "# Remove duplicates\n",
    "articles_df = articles_df.drop_duplicates(subset=['url'])\n",
    "\n",
    "# Save to CSV (keeping this for backup)\n",
    "articles_df.to_csv(\"search_engine_land_articles.csv\", index=False)\n",
    "\n",
    "print(f\"Completed! Scraped {pages_scraped} pages and found {len(articles_df)} unique articles.\")\n",
    "print(f\"Data saved to search_engine_land_articles.csv\")\n",
    "\n",
    "# Print a sample of the articles found\n",
    "print(\"\\nSample of articles found:\")\n",
    "for i, (title, url) in enumerate(zip(articles_df['title'].head(5), articles_df['url'].head(5))):\n",
    "    print(f\"{i+1}. {title}\")\n",
    "    print(f\"   {url}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to PostgreSQL database...\n",
      "Connection successful!\n",
      "Creating database table if it doesn't exist...\n",
      "Table created or already exists.\n",
      "Inserting articles into the database...\n",
      "Successfully inserted 0 new articles into the database.\n",
      "Skipped 3 articles that were already in the database.\n",
      "Database operations completed.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import psycopg\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the database connection string from the environment variable\n",
    "dbconn = os.getenv(\"DBCONN\")\n",
    "\n",
    "if dbconn is None:\n",
    "    print(\"Error: Database connection string not found.\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"Connecting to PostgreSQL database...\")\n",
    "\n",
    "try:\n",
    "    # Establish database connection\n",
    "    conn = psycopg.connect(dbconn)\n",
    "    print(\"Connection successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to the database: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Create a cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create the table if it doesn't exist\n",
    "print(\"Creating database table if it doesn't exist...\")\n",
    "try:\n",
    "    cur.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS search_engine_land_articles(\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            title VARCHAR(500) NOT NULL,\n",
    "            url VARCHAR(500) NOT NULL UNIQUE\n",
    "        );\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    print(\"Table created or already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating table: {e}\")\n",
    "    conn.close()\n",
    "    exit(1)\n",
    "\n",
    "# Simulate a DataFrame (replace this with your actual data loading)\n",
    "articles_df = pd.DataFrame([\n",
    "    {\"title\": \"Article 1\", \"url\": \"https://example.com/1\"},\n",
    "    {\"title\": \"Article 2\", \"url\": \"https://example.com/2\"},\n",
    "    {\"title\": \"Article 3\", \"url\": \"https://example.com/3\"}\n",
    "])\n",
    "\n",
    "# Insert the articles into the database\n",
    "print(\"Inserting articles into the database...\")\n",
    "articles_inserted = 0\n",
    "articles_skipped = 0\n",
    "\n",
    "for _, row in articles_df.iterrows():\n",
    "    try:\n",
    "        # Try to insert the article\n",
    "        cur.execute('''\n",
    "            INSERT INTO search_engine_land_articles (title, url)\n",
    "            VALUES (%s, %s)\n",
    "            ON CONFLICT (url) DO NOTHING;\n",
    "        ''', (row['title'], row['url']))\n",
    "\n",
    "        # Check if a row was inserted\n",
    "        if cur.rowcount > 0:\n",
    "            articles_inserted += 1\n",
    "        else:\n",
    "            articles_skipped += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting article: {e}\")\n",
    "\n",
    "# Commit all the inserts\n",
    "conn.commit()\n",
    "\n",
    "print(f\"Successfully inserted {articles_inserted} new articles into the database.\")\n",
    "print(f\"Skipped {articles_skipped} articles that were already in the database.\")\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"Database operations completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to PostgreSQL database...\n",
      "Connection successful!\n",
      "Creating database table if it doesn't exist...\n",
      "Table created or already exists.\n",
      "Inserting articles into the database...\n",
      "Successfully inserted 0 new articles into the database.\n",
      "Skipped 3 articles that were already in the database.\n",
      "Database operations completed.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import psycopg\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the database connection string from the environment variable\n",
    "dbconn = os.getenv(\"DBCONN\")\n",
    "\n",
    "if dbconn is None:\n",
    "    print(\"Error: Database connection string not found.\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"Connecting to PostgreSQL database...\")\n",
    "\n",
    "try:\n",
    "    # Establish database connection\n",
    "    conn = psycopg.connect(dbconn)\n",
    "    print(\"Connection successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to the database: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Create a cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create the table if it doesn't exist\n",
    "print(\"Creating database table if it doesn't exist...\")\n",
    "try:\n",
    "    cur.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS search_engine_land_articles(\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            title VARCHAR(500) NOT NULL,\n",
    "            url VARCHAR(500) NOT NULL UNIQUE\n",
    "        );\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    print(\"Table created or already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating table: {e}\")\n",
    "    conn.close()\n",
    "    exit(1)\n",
    "\n",
    "# Simulate a DataFrame (replace this with your actual data loading)\n",
    "articles_df = pd.DataFrame([\n",
    "    {\"title\": \"Article 1\", \"url\": \"https://example.com/1\"},\n",
    "    {\"title\": \"Article 2\", \"url\": \"https://example.com/2\"},\n",
    "    {\"title\": \"Article 3\", \"url\": \"https://example.com/3\"}\n",
    "])\n",
    "\n",
    "# Insert the articles into the database\n",
    "print(\"Inserting articles into the database...\")\n",
    "articles_inserted = 0\n",
    "articles_skipped = 0\n",
    "\n",
    "for _, row in articles_df.iterrows():\n",
    "    try:\n",
    "        # Try to insert the article\n",
    "        cur.execute('''\n",
    "            INSERT INTO search_engine_land_articles (title, url)\n",
    "            VALUES (%s, %s)\n",
    "            ON CONFLICT (url) DO NOTHING;\n",
    "        ''', (row['title'], row['url']))\n",
    "\n",
    "        # Check if a row was inserted\n",
    "        if cur.rowcount > 0:\n",
    "            articles_inserted += 1\n",
    "        else:\n",
    "            articles_skipped += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting article: {e}\")\n",
    "\n",
    "# Commit all the inserts\n",
    "conn.commit()\n",
    "\n",
    "print(f\"Successfully inserted {articles_inserted} new articles into the database.\")\n",
    "print(f\"Skipped {articles_skipped} articles that were already in the database.\")\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"Database operations completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAB_V1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
