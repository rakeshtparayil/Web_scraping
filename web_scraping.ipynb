{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Website Meta Data Scraper ===\n",
      "Starting Chrome...\n",
      "Chrome started successfully!\n",
      "Loading URLs from /Users/rakesh/Downloads/cleaning_company.csv...\n",
      "Loaded 5 URLs.\n",
      "\n",
      "Scraping 5 URLs...\n",
      "Scraping (1/5): https://ï»¿https://dubaihousekeeping.com/\n",
      "Scraping (2/5): https://elitemaids.ae/\n",
      "Scraping (3/5): https://justmaid.ae/\n",
      "Scraping (4/5): https://servicemarket.com/en/dubai/cleaning-maid-services\n",
      "Scraping (5/5): https://dubaiclean.com/\n",
      "Scraping complete. Results saved to scraping_results.csv\n",
      "\n",
      "Scraping complete! Results saved to scraping_results.csv\n",
      "Closing Chrome...\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "class MetaScraper:\n",
    "    def __init__(self, chrome_driver_path=None, headless=False):\n",
    "        \"\"\"\n",
    "        Initialize the scraper with browser settings\n",
    "        \n",
    "        Args:\n",
    "            chrome_driver_path (str): Path to chromedriver\n",
    "            headless (bool): Run browser in headless mode if True\n",
    "        \"\"\"\n",
    "        self.options = Options()\n",
    "        \n",
    "        # Configure browser options\n",
    "        if headless:\n",
    "            self.options.add_argument(\"--headless=new\")\n",
    "            \n",
    "        self.options.add_argument(\"--disable-gpu\")\n",
    "        self.options.add_argument(\"--no-sandbox\")\n",
    "        self.options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.options.add_argument(\"--window-size=1920,1080\")\n",
    "        \n",
    "        # Set user agent\n",
    "        self.options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\")\n",
    "        \n",
    "        # Disable images for faster loading\n",
    "        self.options.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2})\n",
    "        \n",
    "        # Setup the driver\n",
    "        try:\n",
    "            service = Service(executable_path=chrome_driver_path)\n",
    "            self.driver = webdriver.Chrome(service=service, options=self.options)\n",
    "            self.driver.set_page_load_timeout(30)\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing Chrome: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def get_meta_data(self, url):\n",
    "        \"\"\"\n",
    "        Scrape meta data from a given URL\n",
    "        \n",
    "        Args:\n",
    "            url (str): Website URL to scrape\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing the meta data\n",
    "        \"\"\"\n",
    "        meta_data = {\n",
    "            'url': url,\n",
    "            'title': '',\n",
    "            'meta_description': '',\n",
    "            'meta_keywords': '',\n",
    "            'status': 'success'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            # Add a small delay to ensure page loads properly\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Get title\n",
    "            meta_data['title'] = self.driver.title\n",
    "            \n",
    "            # Get meta description\n",
    "            try:\n",
    "                meta_description = self.driver.find_element(By.XPATH, \"//meta[@name='description']\")\n",
    "                meta_data['meta_description'] = meta_description.get_attribute('content')\n",
    "            except NoSuchElementException:\n",
    "                meta_data['meta_description'] = 'Not found'\n",
    "            \n",
    "            # Get meta keywords\n",
    "            try:\n",
    "                meta_keywords = self.driver.find_element(By.XPATH, \"//meta[@name='keywords']\")\n",
    "                meta_data['meta_keywords'] = meta_keywords.get_attribute('content')\n",
    "            except NoSuchElementException:\n",
    "                meta_data['meta_keywords'] = 'Not found'\n",
    "            \n",
    "        except TimeoutException:\n",
    "            meta_data['status'] = 'timeout'\n",
    "        except Exception as e:\n",
    "            meta_data['status'] = f'error: {str(e)}'\n",
    "        \n",
    "        return meta_data\n",
    "    \n",
    "    def scrape_sites(self, urls, output_csv='meta_data_results.csv'):\n",
    "        \"\"\"\n",
    "        Scrape multiple URLs and save results to CSV\n",
    "        \n",
    "        Args:\n",
    "            urls (list): List of URLs to scrape\n",
    "            output_csv (str): Output CSV filename\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        total_urls = len(urls)\n",
    "        \n",
    "        for index, url in enumerate(urls):\n",
    "            print(f\"Scraping ({index+1}/{total_urls}): {url}\")\n",
    "            meta_data = self.get_meta_data(url)\n",
    "            results.append(meta_data)\n",
    "            \n",
    "        # Save results to CSV\n",
    "        with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['url', 'title', 'meta_description', 'meta_keywords', 'status']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(results)\n",
    "            \n",
    "        print(f\"Scraping complete. Results saved to {output_csv}\")\n",
    "        \n",
    "        # Convert to DataFrame for easy viewing\n",
    "        df = pd.DataFrame(results)\n",
    "        return df\n",
    "    \n",
    "    def load_urls_from_csv(self, csv_file):\n",
    "        \"\"\"\n",
    "        Load URLs from a CSV file\n",
    "        \n",
    "        Args:\n",
    "            csv_file (str): Path to CSV file containing URLs\n",
    "            \n",
    "        Returns:\n",
    "            list: List of URLs from the CSV\n",
    "        \"\"\"\n",
    "        urls = []\n",
    "        try:\n",
    "            with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "                # Try to determine if the CSV has headers and which column contains URLs\n",
    "                sample = file.read(1024)\n",
    "                file.seek(0)\n",
    "                \n",
    "                if ',' in sample:  # Likely a CSV with multiple columns\n",
    "                    reader = csv.reader(file)\n",
    "                    headers = next(reader, None)\n",
    "                    \n",
    "                    # Try to find a column that might contain URLs\n",
    "                    url_col_idx = 0\n",
    "                    if headers:\n",
    "                        for i, header in enumerate(headers):\n",
    "                            if header.lower() in ['url', 'link', 'website', 'site', 'address']:\n",
    "                                url_col_idx = i\n",
    "                                break\n",
    "                    \n",
    "                    for row in reader:\n",
    "                        if row and len(row) > url_col_idx:\n",
    "                            url = row[url_col_idx].strip()\n",
    "                            if url and not url.startswith('#'):\n",
    "                                if not url.startswith(('http://', 'https://')):\n",
    "                                    url = 'https://' + url\n",
    "                                urls.append(url)\n",
    "                else:  # Likely a simple list of URLs, one per line\n",
    "                    for line in file:\n",
    "                        url = line.strip()\n",
    "                        if url and not url.startswith('#'):\n",
    "                            if not url.startswith(('http://', 'https://')):\n",
    "                                url = 'https://' + url\n",
    "                            urls.append(url)\n",
    "            \n",
    "            return urls\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading URLs from CSV: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the browser\"\"\"\n",
    "        self.driver.quit()\n",
    "\n",
    "\n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Website Meta Data Scraper ===\")\n",
    "    \n",
    "    # Set default ChromeDriver path\n",
    "    driver_path = \"/Users/rakesh/Downloads/chromedriver-mac-arm64/chromedriver\"\n",
    "    \n",
    "    # Make sure chromedriver is executable on Mac\n",
    "    if os.path.exists(driver_path) and sys.platform == 'darwin':  # macOS\n",
    "        try:\n",
    "            import stat\n",
    "            os.chmod(driver_path, os.stat(driver_path).st_mode | stat.S_IXUSR)\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    # CSV file path - CHANGE THIS TO YOUR CSV FILE PATH\n",
    "    csv_path = \"/Users/rakesh/Downloads/cleaning_company.csv\"  # <-- MODIFY THIS LINE with your actual CSV file path\n",
    "    \n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"Error: File not found: {csv_path}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Initialize the scraper\n",
    "    try:\n",
    "        print(\"Starting Chrome...\")\n",
    "        scraper = MetaScraper(chrome_driver_path=driver_path)\n",
    "        print(\"Chrome started successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting Chrome: {str(e)}\")\n",
    "        exit(1)\n",
    "    \n",
    "    try:\n",
    "        # Load URLs from CSV\n",
    "        print(f\"Loading URLs from {csv_path}...\")\n",
    "        websites = scraper.load_urls_from_csv(csv_path)\n",
    "        \n",
    "        if not websites:\n",
    "            print(\"No valid URLs found in the CSV file.\")\n",
    "            exit(1)\n",
    "        \n",
    "        print(f\"Loaded {len(websites)} URLs.\")\n",
    "        \n",
    "        # Scrape the websites\n",
    "        output_filename = \"scraping_results.csv\"\n",
    "        print(f\"\\nScraping {len(websites)} URLs...\")\n",
    "        results_df = scraper.scrape_sites(websites, output_csv=output_filename)\n",
    "        \n",
    "        # Show completion message\n",
    "        print(f\"\\nScraping complete! Results saved to {output_filename}\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nScraping stopped by user.\")\n",
    "    finally:\n",
    "        # Clean up\n",
    "        print(\"Closing Chrome...\")\n",
    "        scraper.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAB_V1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
